* baseline:Bert直接做67
* model1:roberta直接做，68
* run_roberta_5fold：5折交叉验证，68.45

* 多句话输入：很遗憾，未能实现
* 技巧：
  * 原始数据
    * 是否采用24标签数
    * 标签平滑(效果不太理想)
    * 采用6标签数
    * 采用回归方法做(线上得分高，故采用回归来做)
    * 对原始数据处理(见readme.md)
  * 模型
    * 采用BERT
    * 采用roberta
    * 采用LACO
      * 论文：Enhancing Label Correlation Feedback in Multi-Label Text Classification via Multi-Task Learning
      * 主要利用的方法：训练集[cls]text[sep]true label[sep];测试集:[cls]text[sep]
    * 采用chineseBERT
    * 采用macbert
    * 采用roformer(效果比较好)
    * 采用electra
    * 采用nezha
    * 采用Bi-GRU(效果不理想)
    * 采用CNN(效果不理想)
    * 多模型融合（将多个模型的参数相加平均处理，但是要求模型相同）（第一种方式）
    * 多模型融合（投票）(将每个位置的结果相加求平均)（第二种方式）
  * 训练
    * 余弦退火策略
    * 早停策略
    * 分层权重衰减
    * 梯度裁剪原理(torch.nn.utils.clip_grad_norm)一般梯度爆炸或者消失的时候采用
    * 对抗学习(fgm)大概可以提升一点(FDG)效果不太好
    * 数据增强(EDA)
      * 采用回译方法，然后处理每个标签中数据不平衡的问题(效果不太好)
      * 同义词替换(因时间问题并未实现)
    * 知识蒸馏(自蒸馏：两个一样的模型，自己既作为老师也作为学生)  效果不太好
    * 五折交叉验证(有效果，就是训练时间比较长)
  * 采用回归的方法
    * 处理输出结果的策略
      * [-,0.5):0;[0.5,1.5):1;[1.5,2.5):2;[2.5,+]:3  效果不理想
      * 小于0的全部置为0，大于3的全部置为3  可以提升0.0005
      * 将所得到的多个数据文件进行平均处理。也就是模型融合的第二种方式。
      * 